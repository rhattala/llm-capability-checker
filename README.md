# LLM Capability Checker

> **Assess your hardware's ability to run, train, and fine-tune Large Language Models locally**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![.NET](https://img.shields.io/badge/.NET-8.0-blue)](https://dotnet.microsoft.com/)
[![Platform](https://img.shields.io/badge/Platform-Windows%20%7C%20Linux%20%7C%20macOS-lightgrey)](https://github.com/yourusername/llm-capability-checker)

## ğŸ¯ What is this?

LLM Capability Checker is a **free, open-source desktop application** that helps you:

- âœ… **Understand** if your PC can run specific LLM models locally
- âœ… **Estimate** performance (tokens/sec) for different models
- âœ… **Identify** hardware bottlenecks holding you back
- âœ… **Get recommendations** for the best models for your hardware
- âœ… **Plan upgrades** with cost-effective suggestions
- âœ… **Learn** about LLM requirements through educational content

### Why is this useful?

Before downloading a 50GB model and spending hours setting up tools, wouldn't you like to know if it will even run well on your system? This app gives you that answer in under 2 minutes.

## ğŸŒŸ Features

### Core Capabilities

**ğŸ” Hardware Detection**
- Automatic detection of CPU, GPU, RAM, VRAM, Storage
- Framework compatibility check (CUDA, ROCm, DirectML, oneAPI, WSL2)
- Works offline after initial setup

**ğŸ“Š Smart Scoring System**
- Three scores: Inference, Training, Fine-tuning (0-1000 scale)
- Clear ratings: Excellent, Good, Fair, Limited, Poor
- Detailed breakdown showing what contributes to each score

**ğŸ¤– Model Recommendations**
- 50+ popular models in database (Llama, Mistral, Gemma, Phi, DeepSeek, etc.)
- Shows which models work on your hardware
- Performance estimates (tokens/sec)
- Supports all quantization levels (FP32, FP16, 8-bit, 4-bit)

**ğŸ“ˆ Upgrade Advisor**
- Identifies your biggest bottlenecks
- Specific hardware recommendations (GPUs, RAM, Storage)
- Cost-benefit analysis (Budget, Mid-range, High-end)
- "Before and After" comparisons
- Shows which models each upgrade unlocks

**ğŸ“ Educational Mode**
- Beginner-friendly explanations
- Glossary of technical terms
- Interactive tutorials
- "Why does this matter?" tooltips

**âš¡ Optional Benchmarking**
- Quick test (~30 seconds)
- Comprehensive test (~5 minutes)
- Compares real performance vs estimates

### Two Modes

**ğŸ‘¶ Beginner Mode**
- Simplified language
- More explanations
- Guided workflows
- Perfect for newcomers to local LLMs

**ğŸ§‘â€ğŸ’» Advanced Mode**
- Technical details
- Raw specifications
- Full control
- For experienced users

## ğŸš€ Quick Start

### Prerequisites

- **Operating System**: Windows 10/11 (64-bit), Linux (x64), or macOS (x64/ARM64)
- **RAM**: 100MB free
- **Storage**: 50MB
- **Display**: 1024x768 minimum resolution

### Installation

**Option 1: Portable (Recommended)**

1. Download the latest release from [Releases](https://github.com/yourusername/llm-capability-checker/releases)
2. Extract the ZIP file
3. Run `LLMCapabilityChecker.exe` (Windows) or `./LLMCapabilityChecker` (Linux/macOS)

**Option 2: Build from Source**

```bash
# Clone the repository
git clone https://github.com/yourusername/llm-capability-checker.git
cd llm-capability-checker

# Build
dotnet build src/LLMCapabilityChecker/LLMCapabilityChecker.csproj --configuration Release

# Run
dotnet run --project src/LLMCapabilityChecker/LLMCapabilityChecker.csproj
```

### First Run

1. **Launch** the application
2. **Wait** ~2 seconds for hardware detection
3. **View** your scores on the dashboard
4. **Explore** model recommendations
5. **Check** upgrade suggestions (optional)

That's it! No configuration needed.

## ğŸ“– Documentation

### For Users

- **[User Guide](docs/USER_GUIDE.md)** - How to use the app *(Coming soon)*
- **[FAQ](docs/FAQ.md)** - Common questions *(Coming soon)*
- **[Troubleshooting](docs/TROUBLESHOOTING.md)** - Solve common issues *(Coming soon)*

### For Developers

- **[Project Overview](PROJECT_OVERVIEW.md)** - High-level goals and scope
- **[Technical Requirements](TECHNICAL_REQUIREMENTS.md)** - Detailed specifications
- **[Architecture](ARCHITECTURE.md)** - System design and patterns
- **[User Stories](USER_STORIES.md)** - Feature descriptions and acceptance criteria
- **[AI Implementation Guide](AI_IMPLEMENTATION_GUIDE.md)** - Step-by-step build instructions
- **[Contributing](CONTRIBUTING.md)** - How to contribute *(Coming soon)*

### Model Database

- **[Model Database Schema](data/README.md)** - JSON format specification *(Coming soon)*
- **[Adding Models](docs/ADDING_MODELS.md)** - Contribute new models *(Coming soon)*

## ğŸ—ï¸ Project Structure

```
llm-capability-checker/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ LLMCapabilityChecker/        # Main application
â”‚       â”œâ”€â”€ Models/                  # Data models
â”‚       â”œâ”€â”€ ViewModels/              # MVVM ViewModels
â”‚       â”œâ”€â”€ Views/                   # UI (Avalonia XAML)
â”‚       â”œâ”€â”€ Services/                # Business logic
â”‚       â””â”€â”€ Helpers/                 # Utilities
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ LLMCapabilityChecker.Tests/  # Unit tests
â”œâ”€â”€ data/
â”‚   â””â”€â”€ models.json                  # Model database
â”œâ”€â”€ docs/                            # User documentation
â”œâ”€â”€ PROJECT_OVERVIEW.md              # Project goals
â”œâ”€â”€ TECHNICAL_REQUIREMENTS.md        # Tech specs
â”œâ”€â”€ ARCHITECTURE.md                  # System design
â”œâ”€â”€ USER_STORIES.md                  # Feature descriptions
â””â”€â”€ AI_IMPLEMENTATION_GUIDE.md       # Build instructions
```

## ğŸ› ï¸ Technology Stack

- **Framework**: .NET 8
- **UI**: Avalonia UI (cross-platform XAML)
- **Architecture**: MVVM (Model-View-ViewModel)
- **Dependency Injection**: Microsoft.Extensions.DependencyInjection
- **Testing**: xUnit, Moq, FluentAssertions
- **Data Format**: JSON
- **Distribution**: Portable executable

## ğŸ¯ Roadmap

### âœ… Phase 1: Core MVP (Current)
- Hardware detection
- Scoring system
- Model recommendations
- Basic UI

### ğŸš§ Phase 2: Enhancement (In Progress)
- Upgrade advisor
- Educational content
- Optional benchmarking
- Polish and optimization

### ğŸ“‹ Phase 3: Future Features
- Multi-GPU support
- Custom model entry
- Historical tracking
- CLI version
- Community database integration

## ğŸ¤ Contributing

We welcome contributions! Whether you're:

- ğŸ› **Reporting bugs** - Open an issue
- ğŸ’¡ **Suggesting features** - Open an issue with "[Feature Request]"
- ğŸ“ **Improving docs** - Submit a PR
- ğŸ”§ **Writing code** - Check open issues labeled "good first issue"
- ğŸ¤– **Adding models** - Submit PRs to `data/models.json`

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines *(Coming soon)*.

## ğŸ“Š Model Database

The app downloads the latest model database from GitHub on startup. You can also view it locally at `data/models.json`.

**Currently included models**:
- Llama family (1B, 8B, 70B)
- Mistral (7B)
- Gemma 2 (9B)
- DeepSeek Coder (6.7B)
- Phi-4 (14B)
- And more...

Want to add a model? See [docs/ADDING_MODELS.md](docs/ADDING_MODELS.md) *(Coming soon)*.

## ğŸ”’ Privacy

- **100% Local** - No telemetry or tracking by default
- **Opt-in Sharing** - You can choose to share anonymous benchmarks with the community
- **No Account Required** - Works completely offline (after initial model DB download)
- **Open Source** - Audit the code yourself

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Avalonia Team** - Amazing cross-platform UI framework
- **Mozilla Builders** - Inspiration from LocalScore project
- **LLM Community** - Model information and benchmarks
- **Contributors** - Everyone who helps improve this project

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/yourusername/llm-capability-checker/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/llm-capability-checker/discussions)
- **Email**: your.email@example.com *(if you want to provide one)*

## ğŸŒŸ Star History

If you find this project useful, please consider giving it a â­ on GitHub!

---

## Screenshots

### Dashboard
*(Screenshot coming soon)*

### Hardware Details
*(Screenshot coming soon)*

### Model Recommendations
*(Screenshot coming soon)*

### Upgrade Advisor
*(Screenshot coming soon)*

---

## Quick Links

- ğŸ“¦ [Latest Release](https://github.com/yourusername/llm-capability-checker/releases/latest)
- ğŸ› [Report a Bug](https://github.com/yourusername/llm-capability-checker/issues/new?template=bug_report.md)
- ğŸ’¡ [Request a Feature](https://github.com/yourusername/llm-capability-checker/issues/new?template=feature_request.md)
- ğŸ“– [Documentation](docs/)
- ğŸ’¬ [Discussions](https://github.com/yourusername/llm-capability-checker/discussions)

---

**Made with â¤ï¸ for the local LLM community**

*Helping everyone run AI on their own hardware, one score at a time.*
